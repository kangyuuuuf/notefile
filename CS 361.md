

# CS 361 Prob & Stat for Computer Sci

[The poll link](https://pollev.com/hongyeliu713) / [The course link](https://canvas.illinois.edu/courses/24920) / 

#### Data types

**Categorical:** discreet, not have the order (the genre of music)

**Ordinal:** ordered, discreet (the level of school)

**Continuous:** continuous data (temperature)

#### Simple Visualization of Data

**General principles:** must not mislead or distort; Aesthetically pleasing; Clear, convincing; Show message

Bar graph & Histogram

####  Summarizing 1-D continuous data

For a data set $\{x\}$ or annotated as $\{x_i\}$, we summarize with:

**Location Parameters:** mean, median, mode

**Scale Parameters:** $std$, $variance = std^2$, interquartile range

##### Mean

$$
mean(\{x_i\}) = \frac{1}{N}\sum^N_{i=1}x_i \\
mean(\{k \cdot x_i\}) = k \cdot mean(\{x_i\}) \\
mean(\{x_i + c\}) = mean(\{x_i\}) +c
$$

The mean **minimizes** the sum of the squared distance from any real value.
$$
arg\min_\mu (\sum^N_{i=1}(x_i-\mu)^2) = mean(\{x_i\})
$$

##### Standard Deviation($\sigma$)

$$
std(\{x_i\}) = \sqrt{\frac{1}{N} \sum^N_{i=1}(x_i-mean(\{x_i\}))^2} = mean(\{(x_i-mean(\{x_i\}))^2\}) \\
 std(\{k \cdot x_i\})= |k|\cdot std(\{x_i\})\\
 std(\{ x_i + c\})= std(\{x_i\})
$$

At most $\frac{N}{k^2}$ items are $k$ standard deviations away from the mean.

##### Variance($\sigma^2$)

$$
var(\{x_i\}) = \frac{1}{N} \sum^N_{i=1}(x_i-mean(\{x_i\}))^2
$$

#### Interquartile range

$$
iqr(\{k \cdot x_i\}) = |k|\cdot iqr(\{x_i\}) \\

iqr(\{x_i+c\}) = iqr(\{x_i\})
$$



##### Normalized Data

The mean tells where the data set is and the standard deviation tells how spread out it is. If we are interested only in comparing the shape, we could define:
$$
\hat x_i = \frac{x_i-mean(\{x_i\})}{std(\{x_i\})}
$$
And we say $\{\hat x_i\}$ is in standard coordinates.

##### Median

Same feature as mean



#### Permutation

Given a finite number of distinct elements.

A permutation is a particular way of ordering them, and Number of ways of ordering n distinct elements: $n!$

#### K-Permutation

Given a finite number of distinct elements.

K-permutation is a particular way of ordering them with k elements, and Number of ways of ordering k distinct elements with n overall elements: $\frac{n!}{(n-k)!}$

#### Conbinations 

A combination is a choice of k obhjects  out of n distict object $0\le k \le n$, no order needed for the choosen stuff. n choose k can be denoted as $\frac{n!}{k!(n-k)!}$

Note:
$$
(^{n}_k) = (^{\  \ n}_{n-k}) \\
n(^{n-1}_{k-1}) = k(^n_k)
$$


#### Partitions

Given n distinct objects, when we divid them into r disjoint groups.

The total number of partitions is 
$$
\frac{n!}{n_1!n_2! \dots n_r!}
$$

#### Tails and Skews

<img src="CS%20361.assets/image-20220830112831805.png" alt="image-20220830112831805" style="zoom:50%;" />



#### Correlation seen from scatter plots

<img src="CS%20361.assets/image-20220830113139819.png" alt="image-20220830113139819" style="zoom:50%;" />

#### Correlation Coefficient

$$
corr(\{x_i,y_i\}) = \frac{1}{N}\sum_{i=1}^n \hat{x_i} \hat{y_i}
$$

where $\hat{x_i} = \frac{x_i - mean(\{x_i\})}{std(\{x_i\})}$ and $\hat{y_i} = \frac{y_i - mean(\{y_i\})}{std(\{y_i\})}$ 

Notice that $corr(\{x_i,y_i\})=corr(\{y_i,x_i\})$ which is symmetric. Translating the data does not change the correlation coefficient.

Notice that scaling the data may change the sign of the correlation coefficient
$$
corr(\{ax_i+b,cy_i+d\}) = sign(ac) corr(\{x_i,y_i\}) 
$$
The correlation coefficient is bounded within [-1,1]. When the plot shows a straight line, the coefficient is (1,-1). However, we cannot calculate the coefficient if the slope of the line is zero.

We can rewrite the formula into
$$
corr(\{x_i,y_i\}) = \sum_{i=1}^n \frac{\hat{x_i}}{\sqrt N} \frac{\hat{y_i}}{\sqrt N} = v_1^Tv_2 = |v_1||v_2|\cos(\theta)
$$
Since the norm of 2 vector is 1 and cos function is bounded between -1 and 1. Then, it is bounded within [-1,1].

We have the equation.
$$
y^p = rx^p
$$


#### Outcome

An outcome A is a possible result of a random repeatable experiment.

#### Sample space

The sample space, $\Omega$, is the set of all possible mutually exclusive outcomes associated with the experiment. 

#### Event

An event $E$ is a subset of the sample space $\Omega$. 

#### Frequency Interpretation of Probability.

Given an experiment with an outcome A, we can calculate the probability of $A$ by repeating the experiment over and over
$$
P(A) = \lim_{N\rarr \infin} \frac{\text{number of time  $A$ occurs}}{N}
$$
So, $0 \le P(A) \le 1$ and $\sum_{A_{i}\in\Omega} = 1$

If the outcomes have equal probability,
$$
P(E) = \frac{\text{number of outcomes in $E$}}{\text{total number of outcomes in } \Omega}
$$


#### Axiomatic Definition of Probability

A probability function is any function P that maps sets to real number and satisfies the following three axioms:

1. The probability of any event E is non-negative: $P(E) \ge 0$
2. Every experiment has an outcome: $P(\Omega) = 1$

3. The probability of disjoint events is additive: $P(E_1 \cup E_2 \dots \cup E_N) = \sum^N_{i=1}P(E_I)$ if $E_i \cap E_j = \empty$ for all $i \ne j$.

#### Properties of probability

The compliment: $P(E^c )= 1 - P(E)$

The difference: $P(E_1-E_2) = P(E_1) -P(E_1 \cap E_2)$

The Union: $P(E_1 \cup E_2) = P(E_1)+P(E_2) - P(E_1\cap E_2)$ 

### Conditional Probability

$$
P(A|B) = \frac{P(A \cap B)}{P(B)} \text{ where }P(B) \neq 0
$$

Then, we have
$$
P(A\cap B) = P(A|B)P(B)
$$
Combine together, we have the Bayes rule
$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$
Note
$$
P(A\cap B) = P(B \cap A)  \Longrightarrow P(B|A)P(A) = P(A|B) P(B)
$$

### Bayes Rule, Inpendence

#### Total Probability

Lest events $A_1,A_2, \dots, A_n$ be a partition of the sample space( i.e. $A_i $ are disjoint events and their union is the sample space), wth $P(A_i) >0$ for all $i$. Then, 
$$
P(B) = \sum^n _i P(A_i)P(B|A_i)
$$


#### Bayes Rule Using Total Prob

$$
P(A_j | B) = \frac{P(B|A_j)P(A)}{P(B)} = \frac{P(B|A_j)P(A)}{\sum_j  P(B|A_j)P(A_j)}
$$

### Random Variable

The values of a random variable can be either discrete, continuous or mixed. The range of a discrete random variable is a countable set of real numbers. 

#### Three important facts of Random Variables

- Randome variables have probability 
- Rabdom variables can be conditioned on events or other random variables
- Random variables have averages.

**Probability function** for a random variable $X$ is $P(X = x_0) = \sum P(w_i)$

$P(X= x)$ is called the **probability distribution** for all possible $x$.

$P(X=x) $ is also denoted as $P(x)$ or $p(x)$.

$P(X = x) \ge 0$ for all values that $X$ can take, and is 0 everywhere else

The sum of the probability distribution is 1

#### Cumulative distribution

$P(X \le x)$ is called the cumulative distribution function of $X$

$P(X \le x)$ is also denoted as $f(x)$

$P(X \le x)$ is a non-decreasing funciton of $x$

### PDF and Expected Value

The conditional probability distribution of $X$ given $Y$ is
$$
P(x|y) = \frac{P(x,y)}{p(y)} \qquad P(y) \ne 0
$$
Note $P(x,y) = P(X=x \cap Y =y)$.

The joint probability distribution of two random variables $X$ and $Y$ is $P(\{X=x\} \cap \{Y=y\})$



#### Marginal from Joint distri.

We can recover the individual probability distributions from the joint probability distribution
$$
P(x) = \sum_yP(x,y)
$$


The sum of the joint probability distribution
$$
\sum_y \sum_x P(x,y) =1
$$
Random varibale $X$ and $Y$ are independent if
$$
P(x,y) = P(x) P(y) \text{ for all $x$ and $y$}
$$
If X and Y are disjoint, then they cannot be independent.

#### Expected Value



The expected value (or expectation) of a random variable $X$ is
$$
E[X] = \sum_x xP(x)
$$


##### Linearity

$E[X] = \sum _i x_ip_X(x_i)$

$E[kx]= kE[x]$

$E[x+y] = E[x]+E[y]$

$E[kX+c] = kE[X] +c$

#### Variance and standard deviation

The variance of a random variable $X$ is
$$
var[X] = E [(X- E[X])^2]
$$
It is the same as:
$$
var[X] = E[X^2] - E{[X]^2}
$$

For random varibale $X$ and constant $k$ $var[kX] = k^2 var[X]$.

#### Multiple Random Variables

##### Join PMF

- $p_{xy}(x_i,y_j) = P(X = x_i \& Y =y_j)$

##### Marginal probability can be derived from the joint PMF

- $P_x(x_i) = \Sigma_jp_{XY}(x_i,y_j)$
- $P_y(y_j) = \Sigma_ip_{XY}(x_i,y_j)$

##### Conditional probability mass function

- $p_{X|Y}(x_i|y_j) = P(X=x_i |Y = y_j) = \frac{p_{X|Y}(x_i,y_j)}{p_Y(y_j)}$

#### Independence

- $p_{XY}(x_i,y_j) = p_X(x_i)p_Y(y_j) \iff P(X \in B)P(Y\in C)$

##### Variance

$Var(X) = E[(X-E[X])^2] = E[X^2] - E[X]^2$

#### Expected value of a function of $X$

If $f$ is a function of a random variable $X$, then $Y = f(x)$ is a random variable too.

The expected value of $Y = f(X)$ is $E[Y] = E[f(x)] = \sum_x f(x) P(x)$

$E[f(x,y)]=\sum_x \sum_yf(x,y)p(x,y)$  

### Weak Law of large Number

#### Covariance

The convariance of random 2 $X$ and $Y$ is  $cov(X,Y) = E[(X -E(X))(Y-E[Y])]$ 

Note that $cov(X,X) = E[(X-E[x])^2] = var[X]$

Then, we can get a neater expression for covariance ( similar derivation as for variance)
$$
cov(X,Y) = E[XY] - E[X]E[Y] = \sum \sum E[XY] - E[X]E[Y]
$$


The correlation coefficient is normalized covariance. The correlation coefficient is 
$$
corr(X,Y) = \frac{cov(X,Y)}{\sigma_x\sigma_y} = \frac{E[XY]-E[X]E[Y]}{\sigma_x\sigma_y}
$$
Note that independence indicates correlation  = 0. However, correlation = 0 do not indicate independence
$$
var[X+Y] = var[X] + var[Y] + 2 cov(X,Y)
$$
When the correlation coefficient is 0, then $E[XY] -E[X] [Y] = 0 \Longrightarrow E[XY] = E[X]E[Y]$. This is a necessary property of independence of random variables. Or we can say,

Independence $\implies$ Covariance = 0 and Covariance = 0 $\centernot\Longrightarrow$ Independence

If  correlation = 0, then
$$
E[XY] = E[X] E[Y]\\
cov(X,Y) = 0 \\ 
var[X+y] = var[X] +var[Y]
$$

Note if $cov(X,Y) = 0$, we have
$$
E[X+Y] = E[X] + E[Y] \qquad \text{always true}
\\V[X+Y] = V[X]+V[Y] + 2cov(X,Y) \qquad \text{when } cov(X,Y) = 0
\\E[XY] = E[X]E[Y]  \Longleftarrow E[XY]-E[X]E[Y] = cov(X,Y) = 0
\\Corr[X,Y] = 0 \Longleftarrow corr[X,Y] = \frac{cov(X,Y)}{\sigma_X \sigma_Y} = 0
$$
However, we do not have $P(X=x,Y=y) = P(X=x)P(Y=y)$ since $cov(X,Y) = 0$ do not imply independence 

#### Markov’s inequality

For any random variable $X^*$ that only takes $x^* \ge 0$ and constant $a >0$
$$
P(X^* \ge a) \le \frac{E[X^*]}{a}
$$

#### Chebyshev’s inequality

For **any** random variable $X$ and constant $a > 0$
$$
P(|X-E[X]| \ge a ) \le \frac{var[X]}{a^2}
$$
If we let $a = k \sigma$ where $\sigma = std[x]$
$$
P(|X-E[X]| \ge k \sigma ) \le  \frac{1}{k^2}
$$
In words, the probability that $X$ is greater than $k$ standard deviation away from the mean is small.

#### Sample mean and IID samples

We define the sample mean $\bar{X}$ to be the average of $N$ random variables $X_1, \dots, X_N$. If $X_1, \dots, X_N$ are **independent** and have **identical** probability function $P(X)$, then the numbers randomly generated from them are called **IID samples**. The **sample mean** is a random variable.

Assume we have a set of **IID samples** from $N$ random variables $X_1, \dots, X_N$ that have probability function $P(X)$, we use $\bar X$ to denote the sample mean of these **IID samples**
$$
\bar X = \frac{\sum_{i=1}^N X_i}{N}
$$
By linearity of expected value
$$
E[\bar X] = E[\frac{\sum_{i=1}^N X_i}{N}] = \frac{1}{N} \sum_{i=1}^N E[X_i] = E[X]
$$
By the scaling of variance
$$
var[\bar X] = var[\frac{1}{N} \sum_{i=1}^N X_i] = \frac{1}{N^2} var[\sum_{i=1}^N X_i]
$$
Given each $X_i$ has identical $P(x), var[X_i] = var[X]$
$$
var[\bar X] = \frac{var[X]}{N}
$$

#### Weak law of large numbers

Given a random variable $X$ with finite variance, probability distribution function $P(X)$ and the sample mean $\bar X$ of size $N$

For any positive number $\epsilon > 0$ 
$$
\lim_{N \rarr \infin} P(|\bar X - E[X]| \ge \epsilon) = 0
$$
That is: the value of the mean of **IID** samples is very close with a high probability to the expected value of the population when the sample size is very large.

### Discrete Distributions

#### Probability Density Function

For a continuous random variable $X$, the probability that $X= x$ is essentially zero for all (or most) x, so we cannot define $P(X =x)$. Instead, we define the probability density function (pdf) over an infinitesimally small interval $dx$, $p(x)dx = P(X \in [x,x+dx])$. For $a<b$
$$
\int_a^b p(x) dx = P(X \in [a,b])
$$
$p(x)$ **resembels** the probability function of discrete random variables in that 

- $p(x) \ge 0$ for all $x$
- The probability of $X$ taking all possible values is 1.

$p(x)$ differs from the probability distribution function for a discrete random variable in that

- $p(x)$ is not the probability that $X=x$
- $p(x)$ can exceed 1

#### Expectation of Continuous Variables

Expected value of a continuous random variable $X$
$$
E[X] = \int_{-\infin}^{\infin} xp(x) dx
$$
Expected value of function of continuous random variable $Y= f(X)$
$$
E[Y] = E[f(x)] = \int_{-\infin}^{\infin} f(x)p(x) dx
$$
The linearity of the expected value is true for continuous random variables. And the other properties that we derived for variance and covariance also hold for continuous random variables.

#### Distributions

##### Bernoulli distribution

A random variable $X$ is **Bernoulli** if it takes on two values 0 and 1 such that $P(X= 1) = p, P(X= 0) = 1-p$.

Then, $E[x] = p$, $var[X] = p(1-p)$.

Examples: tossing a biased coin; making a free throw; rolling a six-sided die and checking if it show 6

##### Binomial Distribution

A discrete random variable $X$ is binomial if 
$$
P(X = k) = \binom{N}{k}p^k(1-p)^{N-k} \qquad \text{for integer }0\le k\le N
$$
with $E[X] = Np$ and $var[X] = Np(1-p)$

Examples: If we roll a six-sided die N times, how many sixes will we see; What is the sum of N independent and identically distributed Bernoulli trials?

##### Geometric Distribution

A discrete random variable $X$ is geometric if
$$
P(X=k) = (1-p)^{k-1} p \qquad k\ge 1
$$
With $E[X] = \frac{1}{p}$ and $var[X] = \frac{1-p}{p^2}$

Examples: How many rolls of a six-sided die will it take to see the first 6? How many Bernoulli trials must be done before the first 1? How many experiments needed to have the first success?

##### Multinomial Distribution

A discrete r.v. X is multinomial if 
$$
P(X_1=n_1, \dots , X_k=n_k) = \frac{N!}{n_1!n_2!\dots n_k!}p_1^{n_1}p_2^{n_2}\dots p_k^{n_k}
$$
Where $N = n_1+n_2 + \dots + n_k$

##### Poisson Distribution

A discrete random variable $X$ is called Poisson with intensity $\lambda(\lambda >0)$ if 
$$
P(X =k) = \frac{e^{-\lambda}\lambda^k}{k!} \qquad \text{for integer } k \ge 0
$$
$\lambda$ is the average rate of the event's occurrence.

With $E[X] = \lambda$ and $var[X] = \lambda$.

##### Continuous Uniform Distribution

A continuous random variables $X$ is uniform if 
$$
p(x) = \begin{cases}
\frac{1}{b-a} \qquad x\in [a,b]\\
0 \qquad otherwise
\end{cases}
$$
With $E[X] = \frac{a+b}{2}$ and $var[X] = \frac{(b-a)^2}{2}$

##### Exponential Distribution

A continuous random variables $X$ is called exponential distribution if 
$$
p(x) = \lambda e^{-\lambda x}\text{ for }x \ge 0
$$
With $E[X] =\frac{1}{\lambda}$ and $var[X] = \frac{1}{\lambda}$.
