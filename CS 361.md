

# CS 361 Prob & Stat for Computer Sci

[The poll link](https://pollev.com/hongyeliu713) / [The course link](https://canvas.illinois.edu/courses/24920) / 

#### Data types

**Categorical:** discreet, not have the order (the genre of music)

**Ordinal:** ordered, discreet (the level of school)

**Continuous:** continuous data (temperature)

#### Simple Visualization of Data

**General principles:** must not mislead or distort; Aesthetically pleasing; Clear, convincing; Show message

Bar graph & Histogram

####  Summarizing 1-D continuous data

For a data set $\{x\}$ or annotated as $\{x_i\}$, we summarize with:

**Location Parameters:** mean, median, mode

**Scale Parameters:** $std$, $variance = std^2$, interquartile range

##### Mean

$$
mean(\{x_i\}) = \frac{1}{N}\sum^N_{i=1}x_i \\
mean(\{k \cdot x_i\}) = k \cdot mean(\{x_i\}) \\
mean(\{x_i + c\}) = mean(\{x_i\}) +c
$$

The mean **minimizes** the sum of the squared distance from any real value.
$$
arg\min_\mu (\sum^N_{i=1}(x_i-\mu)^2) = mean(\{x_i\})
$$

##### Standard Deviation($\sigma$)

$$
std(\{x_i\}) = \sqrt{\frac{1}{N} \sum^N_{i=1}(x_i-mean(\{x_i\}))^2} = mean(\{(x_i-mean(\{x_i\}))^2\}) \\
 std(\{k \cdot x_i\})= |k|\cdot std(\{x_i\})\\
 std(\{ x_i + c\})= std(\{x_i\})
$$

At most $\frac{N}{k^2}$ items are $k$ standard deviations away from the mean.

##### Variance($\sigma^2$)

$$
var(\{x_i\}) = \frac{1}{N} \sum^N_{i=1}(x_i-mean(\{x_i\}))^2
$$

#### Interquartile range

$$
iqr(\{k \cdot x_i\}) = |k|\cdot iqr(\{x_i\}) \\

iqr(\{x_i+c\}) = iqr(\{x_i\})
$$



##### Normalized Data

The mean tells where the data set is and the standard deviation tells how spread out it is. If we are interested only in comparing the shape, we could define:
$$
\hat x_i = \frac{x_i-mean(\{x_i\})}{std(\{x_i\})}
$$
And we say $\{\hat x_i\}$ is in standard coordinates.

##### Median

Same feature as mean



#### Permutation

Given a finite number of distinct elements.

A permutation is a particular way of ordering them, and Number of ways of ordering n distinct elements: $n!$

#### K-Permutation

Given a finite number of distinct elements.

K-permutation is a particular way of ordering them with k elements, and Number of ways of ordering k distinct elements with n overall elements: $\frac{n!}{(n-k)!}$

#### Conbinations 

A combination is a choice of k obhjects  out of n distict object $0\le k \le n$, no order needed for the choosen stuff. n choose k can be denoted as $\frac{n!}{k!(n-k)!}$

Note:
$$
(^{n}_k) = (^{\  \ n}_{n-k}) \\
n(^{n-1}_{k-1}) = k(^n_k)
$$


#### Partitions

Given n distinct objects, when we divid them into r disjoint groups.

The total number of partitions is 
$$
\frac{n!}{n_1!n_2! \dots n_r!}
$$

#### Tails and Skews

<img src="CS%20361.assets/image-20220830112831805.png" alt="image-20220830112831805" style="zoom:50%;" />



#### Correlation seen from scatter plots

<img src="CS%20361.assets/image-20220830113139819.png" alt="image-20220830113139819" style="zoom:50%;" />

#### Correlation Coefficient

$$
corr(\{x_i,y_i\}) = \frac{1}{N}\sum_{i=1}^n \hat{x_i} \hat{y_i}
$$

where $\hat{x_i} = \frac{x_i - mean(\{x_i\})}{std(\{x_i\})}$ and $\hat{y_i} = \frac{y_i - mean(\{y_i\})}{std(\{y_i\})}$ 

Notice that $corr(\{x_i,y_i\})=corr(\{y_i,x_i\})$ which is symmetric. Translating the data does not change the correlation coefficient.

Notice that scaling the data may change the sign of the correlation coefficient
$$
corr(\{ax_i+b,cy_i+d\}) = sign(ac) corr(\{x_i,y_i\}) 
$$
The correlation coefficient is bounded within [-1,1]. When the plot shows a straight line, the coefficient is (1,-1). However, we cannot calculate the coefficient if the slope of the line is zero.

We can rewrite the formula into
$$
corr(\{x_i,y_i\}) = \sum_{i=1}^n \frac{\hat{x_i}}{\sqrt N} \frac{\hat{y_i}}{\sqrt N} = v_1^Tv_2 = |v_1||v_2|\cos(\theta)
$$
Since the norm of 2 vector is 1 and cos function is bounded between -1 and 1. Then, it is bounded within [-1,1].

#### Outcome

An outcome A is a possible result of a random repeatable experiment.

#### Sample space

The sample space, $\Omega$, is the set of all possible mutually exclusive outcomes associated with the experiment. 

#### Event

An event $E$ is a subset of the sample space $\Omega$. 

#### Frequency Interpretation of Probability.

Given an experiment with an outcome A, we can calculate the probability of $A$ by repeating the experiment over and over
$$
P(A) = \lim_{N\rarr \infin} \frac{\text{number of time  $A$ occurs}}{N}
$$
So, $0 \le P(A) \le 1$ and $\sum_{A_{i}\in\Omega} = 1$

If the outcomes have equal probability,
$$
P(E) = \frac{\text{number of outcomes in $E$}}{\text{total number of outcomes in } \Omega}
$$


#### Axiomatic Definition of Probability

A probability function is any function P that maps sets to real number and satisfies the following three axioms:

1. The probability of any event E is non-negative: $P(E) \ge 0$
2. Every experiment has an outcome: $P(\Omega) = 1$

3. The probability of disjoint events is additive: $P(E_1 \cup E_2 \dots \cup E_N) = \sum^N_{i=1}P(E_I)$ if $E_i \cap E_j = \empty$ for all $i \ne j$.

#### Properties of probability

The compliment: $P(E^c )= 1 - P(E)$

The difference: $P(E_1-E_2) = P(E_1) -P(E_1 \cap E_2)$

The Union: $P(E_1 \cup E_2) = P(E_1)+P(E_2) - P(E_1\cap E_2)$ 

#### Conditional Probability

$$
P(A|B) = \frac{P(A \cap B)}{P(B)} \text{ where }P(B) \neq 0
$$

Then, we have
$$
P(A\cap B) = P(A|B)P(B)
$$
Combine together, we have the Bayes rule
$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$
