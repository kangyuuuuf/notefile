# CS 446

**How do we formulate the learning problem?**

-   Input data/value/vector: $x^{(i)}$
-   Label/output: $y^{(i)}$

**How do we call this process?**

-   Inference
-   Prediction

#### Problem Class

-   Unsupervised learning
-   Supervised learning
-   Sequence learning
-   Reinforcement learning

### Nearest Neighbor-Basics

-   Dataset: $D = \{(x^{(i)},y^{(i)})\}^N_{i=1}$
-   New datapoint: $x$
-   Label of new data point: $y$

How to choose $y$?

$y = y^{(k)}$ where $k = arg\min_{i\in \set{1, \dots , N}} ||x^{(i)}-x ||^2_2 = arg\min_{i\in \set{1, \dots , N}} d(x^{(i)},x)$

We will that $k$ nearest neighbors. And take the majority vote or the average of nearest neighbors.

We can neasure the closeness or nearby by distance measurement, such as Euclidean distance, Hamming distance, cosine distance, etc.

##### Shortcomings

-   Computationally expensive
-   Curse of dimensionality. Nearest neighbor breaks down in high-dimensional spaces because the “neighborhood” become very large
-   Memory issue
-   Sensitive to outliers and easily fooled by irrelevant attributes.

### Probability and Estimation

#### Conditional Probability

$$
P(A|B) = \frac{P(A \cap B)}{P(B)} \iff P(A \cap B) = P(A|B)P(B) \iff P(A \cap B) = P(B|A)P(A)
$$

#### Bayes Rule

$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$

We write $P(A)$ as Prior and $P(A|B)$ as Posterior

#### Joint Probability Distribution

Be smart about how we estimate probabilities from sparse data

-   Maximum Likelihood Estimation
-   Maximum a Posteriori Estimation

Be smart on how to model the joint distributions

-   Conditional Independence
-   Graphical models

#### Principles for Estimating Probabilities

##### Maximum Likelihood Estimate(MLE)

Choose $\theta$ that maximizes the probability of observed data
$$
\hat \theta = arg \max_\theta P(D|\theta)
$$

#####  Maximum a Posteriori (MAP) Estimate

Choose $\theta$ that is most probable given prior probability of $\theta$ and observed data
$$
\hat \theta = arg \max_\theta P(\theta|D) = arg\max_\theta \frac{P(D| \theta)P(\theta)}{P(D)} = arg\max_\theta P(D|\theta)P(\theta)
$$

#### Dirichlet Distribution

If prior is Dirichlet distribution,
$$
P(\theta) = \frac{\theta_1^{\beta_1-1}\theta_2^{\beta_2-1}\dots\theta_m^{\beta_m-1}}{B(\beta_1,\dots, \beta_m)} \sim Dirichlet(\beta_1,\dots, \beta_m)
$$

### Introduction to Optimization

The problem more generally:
$$
min_w \quad f_0(w) \\

s.t.\quad f_i(w) \le0 \quad \forall i \in \set{1,\dots,C} 
$$

#### Least Squares Program

$$
\min_w \frac{1}{2} \sum_{(x^{(i)},y^{(i)}) pin D} (y^{(i)}-\phi(x^{(i)})^Tw)^2
$$

#### Linear Program

$$
\min_w c^Tw \quad s.t. \quad Aw<b
$$

#### Convex Program

When all $f_i$ convex
$$
\min_w f_0(w) \quad s.t \quad f_i(w) \le0 \quad \forall i \in \set{1,\dots,C}
$$
Convex Set: A set is convex if for any two points $w_1,w_2$ in the set, the line segment $\lambda w_1+(1-\lambda)w_2$ for $\lambda \in [0,1]$ also lines in the set.

Convex Function: A function $f$ is convex if its domain is a convex set and for any points $w_1,w_2$ in the domain and any $\lambda \in [0,1]$
$$
f((1-\lambda)w_1+\lambda w_2) \le (1-\lambda)f(w_1)+ \lambda f(w_2)
$$
If $f$ is differentiable, then $f$ is convex if and only if its domain is convex and $f(w_1)\ge f(w_2)+\triangledown f(w_2)^T(w_1-w_2) \forall w_1,w_2$ in the domain.

If $f$ is differentiable, then $f$ is convex if and only if its domain is convex and $\forall w_1,w_2$ in the domain
$$
(\triangledown f(w_1)-\triangledown f(w_2))^T(w_1-w_2)\ge 0 \qquad \text{monotone mapping}
$$
If $f$ is twice differentiable, then $f$ is convex if and only if its domain is convex and $\triangledown^2f(w)  \succeq 0 \forall w$ in the domain. 

<img src="CS%20446.assets/image-20230125174614808.png" alt="image-20230125174614808" style="zoom:50%;" />

#### Optimality of convex optimiztion

$$
\triangledown f = 0\  \&\ \triangledown^2 f \ge 0 \implies \text{local min}
$$

A point $w^*$ is locally optimial if $f(w^*) \le f(w)\  \forall w$ in  a neighborhood of $w^*$; globally optimal if $f(w^*) \le f(w)\  \forall w$

#### Algorithm

We start with initial guess $w$, iterate $k=1,2,3, \dots$

-   Select direction $d_k$ and stepsize $\alpha_k$ 
-   $w \leftarrow w+\alpha_kd_k$
-   check whether we should stop

#### Choose direction

Steepest descent: $d_k = - \triangledown f(w_k) $

Scaled gradient: $d_k = - D_k\triangledown f(w_k)$ for $D_k \succ 0$

-   Newton’s methodL $D_k = [\triangledown^2 f(w_k)]^{-1}$

#### Choose stepsize

Exact: $\alpha_k = arg\min_{\alpha\ge0}f(w_k+\alpha d_k)$

Constant: $\alpha_k=1/L$  for suitable $L$

Diminishing: $\alpha_k \rightarrow 0$ but $\sum_k \alpha_k = \infin$ (e.g., $\alpha_k = 1/k$)

Armijo Rule: scary with $\alpha = s$ and continue with $\alpha = \beta s, \alpha = \beta^2s, \dots$, until $\alpha = \beta^ms$ falls within the set of $\alpha$ with
$$
f(w_k+\alpha d_k) = f(w_k) \le \sigma \alpha \triangledown f(w_k)^T d_k
$$


#### Convergence time

 If Stong convexity 
$$
k \ge O(\log(1/\epsilon))
$$
If not
$$
k \ge O(1/ \epsilon)
$$

#### Optimal?

Polyak’s method (heavy ball)
$$
w_{k+1} = w_k -\alpha \triangledown f(w_k)+\beta_k(w_k-w_{k-1})
$$
Momentum method in deep learning
$$
v_{k+1} = \beta v_k +\triangledown f(w_k)\\
w_{k+1} = w_k - \alpha v_{k+1}
$$

#### Analysis

Batch gradient descent:

-   Convergence rate: $O(\log 1 / \epsilon)$
-   Iteration complexity: linear in $|D|$

Stochastic gradient descent:

-   Convergence rate: $O( 1 / \epsilon)$
-   Iteration complexity: independent  in $|D|$, linear in $|B_k|$

### Principal Component Analysis & Singular Value Decomposition

The goal of PCA: Find that lower dimensional linear subspace in which the projected data have the highest variance
$$
\max_{w_1, \dots, w_d: w_i^Tw_j = \delta_{ij}}\sum_{i=1}^d w_i^T\Sigma w_i
$$
Algorithm:

-   Work sequentially one vector at a time
-   Compute a matrix eigenvalue decomposition

1.  Collect all subspace direction
    $$
    U = [w_1 \  \dots\ w_d]
    $$
    
2.  Project points into subspace
    $$
    \hat x = U^T(x-\mu)
    $$
    
3.  Approximately reconstructed data
    $$
    \tilde x = U\hat x + \mu
    $$
    

##### Alternnative view of PCA:

PCA finds the axis which minimizes the sum of squared distances from points to their orthogonal projections on that axis 
$$
\min_{w:||w||^2_2=1}\frac1{|D|}\sum_{i=1}^{|D|}||x^{(i)} - ww^Tx^{(i)}||^2_2 \implies \min_{w:||w||^2_2=1}Tr(\Sigma) - Tr(\Sigma ww^T) \implies \min_{w:||w||^2_2=1} w^T \Sigma w
$$


#####  Frobenius norm:

$$
||A||^2_F = \sum_{i,j} a^2_{i,j} = Tr(A^TA)
$$

where $Tr(M) = \sum_{i=1}^dM_{ii}$ and $M$  is a $d\times d$ matrix.

##### Rewriting the Objective

$$
\frac1{|D|}\sum_{i=1}^{|D|}||x^{(i)} - ww^Tx^{(i)}||^2_2 \implies \frac1{|D|}||\hat X - ww^T\hat X||^2_F\\
\implies \frac1{|D|} Tr((P\hat X)^T(P\hat X)) \qquad \text{where } P = I - ww^T \\
\implies Tr(\Sigma P)
$$

#### Singular Value Decomposition to compute PCA

Computational cheap than find eigenvector directly.
$$
\frac{1}{|\sqrt D|} \bar X = USV^T
$$
We get 
$$
\Sigma = USV^TVsU^T
$$
 where the left singular vectors $U$ of $\frac{1}{|\sqrt D|}$ are needed

### K-Means Clustering

kMeans/Lloyd’s Algorithm:

-   Initialize: prick $K$ random points as cluster centers $\mu_k$
-   Iterate:
    -   Assign data points $x^{(i)}$ to closest cluster center according to some metric
    -   updat=e the cluster center to be the average of its assignment change
    -   Stopping criterion: when no points’ assignments change

What cost function does kMeans optimize?
$$
\min_{\mu} \min_{r} \sum_{i \in D} \sum _{k=1}^K\frac{1}{2}r_{ik}||x^{(i)} - \mu_k||^2_2 \quad s.t. \left \{ \begin{array}{rcl}
 r_{ike} \in \{0,1\} \qquad \forall i,k \\
 \sum^K_{k=1} r_{ik} = 1 \qquad \forall i

\end{array}\right.
$$
