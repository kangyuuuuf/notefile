# CS 446

**How do we formulate the learning problem?**

-   Input data/value/vector: $x^{(i)}$
-   Label/output: $y^{(i)}$

**How do we call this process?**

-   Inference
-   Prediction

#### Problem Class

-   Unsupervised learning
-   Supervised learning
-   Sequence learning
-   Reinforcement learning

### Nearest Neighbor-Basics

-   Dataset: $D = \{(x^{(i)},y^{(i)})\}^N_{i=1}$
-   New datapoint: $x$
-   Label of new data point: $y$

How to choose $y$?

$y = y^{(k)}$ where $k = arg\min_{i\in \set{1, \dots , N}} ||x^{(i)}-x ||^2_2 = arg\min_{i\in \set{1, \dots , N}} d(x^{(i)},x)$

We will that $k$ nearest neighbors. And take the majority vote or the average of nearest neighbors.

We can neasure the closeness or nearby by distance measurement, such as Euclidean distance, Hamming distance, cosine distance, etc.

##### Shortcomings

-   Computationally expensive
-   Curse of dimensionality. Nearest neighbor breaks down in high-dimensional spaces because the “neighborhood” become very large
-   Memory issue
-   Sensitive to outliers and easily fooled by irrelevant attributes.

### Probability and Estimation

#### Conditional Probability

$$
P(A|B) = \frac{P(A \cap B)}{P(B)} \iff P(A \cap B) = P(A|B)P(B) \iff P(A \cap B) = P(B|A)P(A)
$$

#### Bayes Rule

$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$

We write $P(A)$ as Prior and $P(A|B)$ as Posterior

#### Joint Probability Distribution

Be smart about how we estimate probabilities from sparse data

-   Maximum Likelihood Estimation
-   Maximum a Posteriori Estimation

Be smart on how to model the joint distributions

-   Conditional Independence
-   Graphical models

#### Principles for Estimating Probabilities

##### Maximum Likelihood Estimate(MLE)

Choose $\theta$ that maximizes the probability of observed data
$$
\hat \theta = arg \max_\theta P(D|\theta)
$$

#####  Maximum a Posteriori (MAP) Estimate

Choose $\theta$ that is most probable given prior probability of $\theta$ and observed data
$$
\hat \theta = arg \max_\theta P(\theta|D) = arg\max_\theta \frac{P(D| \theta)P(\theta)}{P(D)} = arg\max_\theta P(D|\theta)P(\theta)
$$

#### Dirichlet Distribution

If prior is Dirichlet distribution,
$$
P(\theta) = \frac{\theta_1^{\beta_1-1}\theta_2^{\beta_2-1}\dots\theta_m^{\beta_m-1}}{B(\beta_1,\dots, \beta_m)} \sim Dirichlet(\beta_1,\dots, \beta_m)
$$

### Introduction to Optimization

The problem more generally:
$$
min_w \quad f_0(w) \\

s.t.\quad f_i(w) \le0 \quad \forall i \in \set{1,\dots,C} 
$$

#### Least Squares Program

$$
\min_w \frac{1}{2} \sum_{(x^{(i)},y^{(i)}) pin D} (y^{(i)}-\phi(x^{(i)})^Tw)^2
$$

#### Linear Program

$$
\min_w c^Tw \quad s.t. \quad Aw<b
$$

#### Convex Program

When all $f_i$ convex
$$
\min_w f_0(w) \quad s.t \quad f_i(w) \le0 \quad \forall i \in \set{1,\dots,C}
$$
Convex Set: A set is convex if for any two points $w_1,w_2$ in the set, the line segment $\lambda w_1+(1-\lambda)w_2$ for $\lambda \in [0,1]$ also lines in the set.

Convex Function: A function $f$ is convex if its domain is a convex set and for any points $w_1,w_2$ in the domain and any $\lambda \in [0,1]$
$$
f((1-\lambda)w_1+\lambda w_2) \le (1-\lambda)f(w_1)+ \lambda f(w_2)
$$
If $f$ is differentiable, then $f$ is convex if and only if its domain is convex and $f(w_1)\ge f(w_2)+\triangledown f(w_2)^T(w_1-w_2) \forall w_1,w_2$ in the domain.

If $f$ is differentiable, then $f$ is convex if and only if its domain is convex and $\forall w_1,w_2$ in the domain
$$
(\triangledown f(w_1)-\triangledown f(w_2))^T(w_1-w_2)\ge 0 \qquad \text{monotone mapping}
$$
If $f$ is twice differentiable, then $f$ is convex if and only if its domain is convex and $\triangledown^2f(w)  \succeq 0 \forall w$ in the domain. 

<img src="CS%20446.assets/image-20230125174614808.png" alt="image-20230125174614808" style="zoom:50%;" />