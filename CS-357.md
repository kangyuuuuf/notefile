# CS 357

## Errors and Complexity

#### Absolute and Relative Error

We define that: Approximate Result = True Value + Error. Then, we can get our absolute error:
$$
Absolute Error = |{x-\hat{x}}|.
$$
And, the relative error:
$$
Relative Error = \frac{Absolute Error}{|{x}|} =\frac{|{x-\hat{x}}|}{|{x}|}
$$

#### Significant Digits/Figures

**Significant figures** of a number are digits that carry meaningful information. We define an approximate result $\hat{x}$ has $n$ **significant figures** of a true value $x$ if the absolute error has zeros in the first n decimal place counting from the leftmost nonzero (leading) digit pf $x$, followed by a digit from 0 to 4. Here is an example.

Assume $x=3.141592653$ and suppose $\hat{x}$ is the approximate result:
$$
\hat{x} = 3.14159 \rarr |{x-\hat{x}}|=0.000002653\rarr\hat x \text{ has 6 significant figures}.
$$

$$
\hat{x} = 3.1415 \rarr |{x-\hat{x}}|=0.000092653\rarr\hat x \text{ has 4 significant figures}.
$$

## Floating Point

#### Floating Point Numbers

A floating-point number can represent numbers of different orders of magnitude(very large and very small) with the same number of fixed digits.

More formally, we can define a floating-point number $x$ as:
$$
x=±q⋅2^m
$$


where:

- ± is the sign
- q is the significand
- m is the exponent

A number xx in a normalized binary floating-point system has the form
$$
x =±1.b_1b_2b_3\dots b_n \times 2^m = ±1.f \times2^m
$$
where **Digits**: $b_i \in {0,1}$,**Exponent range:** Integer $m\in [L,U]$, **Precision:** $p=n+1$, **Smallest positive normalized floating-point number:** $2^L$, **Largest positive normalized floating-point number:** $2^{U+1}(1-2^{-p})$

Outside the range will be considered as **overflow**.

#### Machine Epsilon

**Machine epsilon** ($\epsilon_m$) is defined as the distance (gap) between 1 and the next largest floating-point number. It does not depend on the exponent. 
$$
\epsilon_m = 2^{-n}
$$

## Rounding and Cancellation

Consider a real number in normalized floating-point form:
$$
x=\pm 1.b_1b_2b_3\dots b_n\dots\times 2^m
$$
Without loss of generality, let us assume $x$ is a positive number. In this case, we have:
$$
x_-=\pm 1.b_1b_2b_3\dots b_n\times 2^m
$$

$$
x_+=\pm 1.b_1b_2b_3\dots b_n\times 2^m + 0.\text{(n-1 bits of 0)1} \times 2^m
$$

![rounding_table](CS-357.assets/rounding_table.png)

#### Roundoff Errors

The difference between $x_-$ and $x_+$ is  $\epsilon_m \times 2^m$.

Hence we can use machine epsilon to bound the error in representing a real number as a machine number.

**Absolute error:**
$$
|{fl(x)-x}| \le |{x_+-x_-}| = \epsilon_m \times 2^m
$$
**Relative error:**
$$
\frac{|{fl(x)-x}|}{|{x}|} \le \frac{\epsilon_m \times 2^m}{|{x}|}
$$

#### Floating Point Addition

The basic idea of adding is 

1.  Bring both numbers to a common exponent
2. Do grade-school addition from the front, until you run out of digits in your system
3. Round the result

There is no loss of significant digits with floating-point addition.

#### Floating Point Subtraction and Cancellation

Floating-point subtraction works much the same way that addition does. However, problems occur when you subtract two numbers of similar magnitude. There is an example
$$
a = 1.1011???? \times 2^1,b = 1.1010???? \times 2^1
$$

$$
a-b =0.0001???? \times 2^1
$$

Although the floating-point number will be stored with 4 digits in the fractional, it will only be accurate to a single significant digit. This loss of significant digits is known as **catastrophic cancellation**.

## Taylor Series

#### Infinite Taylor Series Expansion

A Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function’s derivatives at a single point. The Taylor series expansion about $x=x_0$ of a function $f(x)$ that is infinitely differentiable at $x_0$ is the power series
$$
f(x_0)+\frac{f'(x_0)}{1!}(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2+\frac{f'''(x_0)}{3!}(x-x_0)^3 + \dots
$$
Then, we can use summation notation 
$$
\sum_{k=0}^{\infin}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k.
$$

#### Error Bound when Truncating a Taylor Series

Suppose that $f(x)$ is an $n+1$ times differentiable function of $x$, and $T_n(x)$ is the Taylor polynomial of degree $n$ for $f(x) $ centered at $x_0$. Then When $h=|{x-x_0}| \rarr0$, we obtain the truncation error bound by
$$
|{f(x)-T_n(x)}|\le C \cdot h^{n+1}=O(h^{n+1})
$$

#### Taylor Remainder Theorem

Suppose that $f(x)$ is an $n_1$ times differentiable function of $x$. Let $R_n(x)$ denote the difference $f(x)$ and the Taylor polynomial of degree $n$ for $f(x)$ centered at $x_0$. Then
$$
R_n(x) = f(x)-T_n(x)= \frac{f^{n+1}(\mathcal{E})}{(n+1)!}(x-x_0)^{n+1}
$$
for some $\mathcal{E}$ between $x$ and $x_0$. Thus, the constant $C$ mentioned above is 
$$
\max_{\cal E} \frac{|{f^{(n+1)}(\cal E)}|}{(n+1)!}
$$

#### Asymptotic behavior of the error

Let’s say we have $f(x)$ approximated using $t_n(x)$. Suppose the given interval is h1h1 between $x_0$ and $x$ and the error associated with it is $e_1$. Let’s say we have another interval $h_2$ and we need to find the error $e_2$ associated with it.

Using the formula $e=O(h^{n+1})$, we get
$$
e_1 \propto h_1^{n+1}\\
e_2 \propto h_2^{n+1}\\
\frac{e_1}{e_2} = (\frac{h_1}{h_2})^{n+1}\\
e_2=(\frac{h_2}{h_1})^{n+1}e_1
$$

#### Implement Taylor Series in Python

Here we need to import **sympy**, a package for symbolic computation with python.

```python
import sympy as sp
# We need to declear a variable name x
sp.var("x")
# Then, the x will become a variable instead of a char or name.
# We can create a function with variable x.
g = sp.sin(sp.sqrt(x) + 2) ** 2 
# Take a derivative, using .diff(x,the time of diff)
g.diff(x,2)
# Use .sub(x,...) and .evalf() to evaluate the expression for x = 1.
g.subs(x,1).evalf()
# print 0.019914856674817
```

After knowing the basic function of simply, we can implement our Taylor function:

~~~python
def taylor_ser(n,x0,f):
  tn = 0;
  for i in range(n+1):
      tn += f.diff(x, i).subs(x, x0)/factorial(i) * (x-x0)**i
  return tn;
# find the apporixmation
def taylor_appox(value,f):
  return f.subs(x,value).evalf()
~~~

## Random Numbers and Monte Carlo Methods

#### Randomness

```python
import numpy as np

np.random.seed(seed)						# fix the seed of generator
np.random.randint(low,high)			# random integer 
np.random.rand(num)							#	random float number within 0 to 1

np.random.choice([0,1])					# random 0,1 
```

#### Linear Congruential Generator

A **linear congruential generator** (LCG) is pseudorandom number generator of the form:
$$
x_k = (ax_{k-1}+c)\mod M
$$
where $a$ and cc are given integers and $x0$ is called the **seed**. The period of an LCG cannot exceed $M$. The quality depends on both aa and $c$, and the period may be less than $M$ depending on the values of $a$ and $c$.

#### Monte Carlo Methods

**Monte Carlo methods** are algorithms that rely on repeated random sampling to approximate a desired quantity. Monte Carlo methods are typically used in modeling the following types of problems:

- Nondeterministic processes
- Complicated deterministic systems and deterministic problems with high dimensionality (e.g., Monte Carlo integration)

 Therefore, the asymptotic behavior of the Monte Carlo method is $O(\frac{1}{\sqrt{n}})$ where n is the number of samples.
$$
err \rarr \frac{1}{\sqrt{n}}Z
$$

## Vector, Matrices, and Norms

The concept can follow the link [Course Note](https://courses.grainger.illinois.edu/cs357/sp2022/notes/ref-8-vec-mat.html). 

The $\infty$-norm is the $\max \sum^n_{j=0} x_{i,j}$ which is the max of abs sum of a row.

The 1-norm is the max of abs sum of a column.



## Linear System of Equation

 

Useful libraries for this notebook:

```PYTHON
import numpy as np
import numpy.linalg as la
import scipy.linalg as sla
import scipy.sparse as sparse


import matplotlib.pyplot as plt
%matplotlib inline

from PIL import Image

import seaborn as sns
sns.set(font_scale=2)
plt.style.use('seaborn-whitegrid')

from time import time
```

#### Transforming images using linear operators

```python
img = Image.open('ssn.png')
xmat = (255 - np.asarray(img).max(axis=2))/255

x = xmat.flatten()
x.shape # The shape is (x,)

```

We can construct a “blur ” matrix, this will blur our picture

```python
def blur_operator(m,n,radius):
    imat, jmat = np.meshgrid(np.arange(m), np.arange(n), indexing='ij')
    ivec = np.atleast_2d(imat.flatten())
    jvec = np.atleast_2d(jmat.flatten())
    A = np.fmax(0, 1 - np.sqrt((ivec.T - ivec)**2 + (jvec.T - jvec)**2)/radius)
    A /= A.sum(axis=1)
    return A
```

~~~python
A = blur_operator(xmat.shape[0],xmat.shape[1],5)
A.shape #(x,x)
~~~

Know, we have blur picture b, we know the transform matrix A, we can get our picture back using **np.linalg.solve**

```python
x_solve1 = la.solve(A,b)
x_solve1.shape
```

We can add some noise to our blur picture, this will make the process not invertible.

~~~python
b_noisy = b + 1e-3 * np.random.rand(b.size)

x_noisy = la.solve(A,b_noisy)

plt.imshow(x_noisy.reshape(xmat.shape))
~~~

#### Solving Triangular Systems

We have forward-substitution and backward-substitution. Using substitution, the running time of calculation is faster than directly finding the solution. Although the running time of finding LU takes more time than finding a solution, it is extremely useful when we need to calculate different variables when we have the same matrix.

```python
def my_forward_substitution(M,b):
    n = len(b)
    y = np.zeros(n)
    for i in range(n):
        tmp = b[i]
        for j in range(i):
            tmp -= y[j]*M[i,j]
        y[i]=tmp/M[i,i]
    return y

def my_backward_substitution(M,b):
    n = len(b)
    y = np.zeros(n)
    for i in range(n-1, -1, -1):
        tmp = b[i]
        for j in range(i+1, n):
            tmp -= y[j]*M[i,j]
        y[i] = tmp/M[i,i]
    return y
```

Using scipy, we can get the LU decomposition. Using **solve_triangular**,  we can get the solution of triangular matrix

```python
import scipy.linalg as sla
P, L, U = sla.lu(A)

y = sla.solve_triangular(L, P.T@b, lower=True)

x_solve = sla.solve_triangular(U, y)
```

## Sparse Matices

A $n\times n$ matrix is called dense of it has $O(n^2)$ non-zero entries.

A $n\times n$ matrix is called sparse of it has $O(n)$ non-zero entries.

**COO** (Coordinate Format) sores arrays of row indices, column indices, and the corresponding non-zero data values in any order. A COO stores three arrays, which are data, row, and col. They will have the same length and correspond a value in value for index n. For example, for arbitrary index $x$, (data = 1, row = 2, col = 1) means the data in row 2, col 1 is 1. 

**CSR** (Compressed Spare Row) encodes rows offsets, column indices, and the corresponding non-zero data values. A CSR also stores three arrays, which are data, col, row-try. Data and col will have the same length, while rowptr represent the number of value accumulative. For example, rowptr = [0, 2, 5, 9] means the first row has 2 non-zero entry and the second row have 3 non-zero entry, etc.

The example code of CSR.

```python
import numpy as np
def csr_mat_vec(A, x):
  Ax = np.zeros_like(x)
  for i in range(x.shape[0]):
    for k in range(A.rowptr[i], A.rowptr[i+1]):
      Ax[i] += A.data[k]*x[A.col[k]]
  return Ax

```

## Condition Numbers

The **condition number** of a square nonsingular matrix **A** is defined by $cond(A) = ||A||\ ||A^{-1}||$ which is also the condition number associated with solving the linear system $Ax=b$. Large condition number $\rarr$ **ill-conditioned**.

A is singular $\rarr$ $cond(A) = \infin$

We have the formula:
$$
\frac{||\Delta x||}{||x||} \leq cond(A) \frac{||\Delta b||}{||b||}
$$
Where  $\frac{||\Delta x||}{||x||}$ is the output and $\frac{||\Delta b||}{||b||}$ is the input.

We can also write:
$$
 \frac{||\Delta b||}{||b||} \leq cond(A)\frac{||\Delta x||}{||x||}
$$

#### Residual vs Error

The **residual vector r** of approximate solution $\hat x$ for the linear system $Ax = b$ is defined as $r = b-A\hat x$.
$$
r =b-(b+\Delta b) = -\Delta b 
$$
Then, substitute with r, 
$$
\frac{||\Delta x||}{||x||} \leq cond(A) \frac{||r||}{||b||}
$$
Also, 
$$
||\Delta x|| \leq cond(A) \frac{||r||}{||A||}
$$


Gaussian Elimination (with Partial Pivoting) is Guaranteed to Produce a Small Residual.

#### Accuracy Rule of Thumb

In IEEE double-precision, $ϵ_{mach}≈2.2\times 10^{−16}$, which means the entries in A and b are accurate to $|\log_{10}⁡(2.2×10^{-16})|$≈16 decimal digits.

Then, using the rule of thumb, we know the entries in $\hat x$ will be accurate to about $16−10=6$ decimal digits.

## Eigenvalues and Eigenvectors

An **eigenvalue** of an $n×n$ matrix $A$ is a scalar $\lambda$ such that $Ax= \lambda x$ for some non-zero vector $x$.

We have $\det(A - \lambda I)=0$.

#### Eigenvalues of a Shifted Matrix

$$
(A-\sigma I)x= Ax-\sigma Ix = \lambda x - \sigma x = (\lambda - \sigma) x
$$

#### Eigenvalues of an Inverse

$$
Ax = \lambda  x \rarr A^{-1}Ax = \lambda A^{-1}x \rarr x = \lambda A^{-1}x \rarr A^{-1} x = \frac{1}{\lambda} x 
$$

#### Eigenvalues of a Shifted Inverse

$$
(A-\sigma I)^{-1} x = \frac{1}{\lambda-\sigma} x
$$

If an $n \times n$ matrix $A$ is diagonalizable, then we can write an arbitrary vector as a linear combination of the eigenvectors of $A$. Let $u_1,u_2,\dots,u_n$ be $n$ linearly independent eigenvectors of $A$; then an arbitrary vector $x_0$ can be written:
$$
x_0 = \alpha_1 u_1 + \alpha_2 u_2 + \dots + \alpha_n u_n
$$
If we apply the matrix $A$ to $x_0$:
$$
Ax_0 = A\alpha_1 u_1 + A\alpha_2 u_2 + \dots + A\alpha_n u_n \\ =  \alpha_1\lambda_1 u_1 + \alpha_2\lambda_2 u_2 + \dots + \alpha_n\lambda_n u_n \\ =\lambda_1 (\alpha_1 u_1 + \alpha_2\frac{\lambda_2}{\lambda_1} u_2 + \dots + \alpha_n\frac{\lambda_n}{\lambda_1} u_n)
$$
If we repeatedly apply $A$ We have
$$
A^kx_0 = \lambda_1^k (\alpha_1 u_1 + \alpha_2(\frac{\lambda_2}{\lambda_1})^k u_2 + \dots + \alpha_n(\frac{\lambda_n}{\lambda_1})^k u_n)
$$
In this case, we have $|\lambda_1| \gt|\lambda_2| \geq |\lambda_3| \geq \dots \geq |\lambda_n|$.

Notice that when $k \rarr \infin$, $(\frac{\lambda_i}{\lambda_1})^k$ towards to 0 for $i>1$. Therefore, we call $\alpha_1 u_1$ dominate term. This implies
$$
\lim_{k\rarr\infin} \frac{A^k x_0}{\lambda_1^k} = \alpha_1 u_1
$$
The largest eigenvalue could be positive, negative, or a complex number. In each case we will have:
$$
\lambda_1 \gt 0 \Rightarrow x_k \approx \frac{\alpha_1 u_1}{||\alpha_1 u_1||} \\
\lambda_1 \lt 0 \Rightarrow x_k \approx (-1)^k\frac{\alpha_1 u_1}{||\alpha_1 u_1||}\\
\lambda_1 = re^{i\theta} \Rightarrow x_k \approx e^{i\theta} \frac{\alpha_1 u_1}{||\alpha_1 u_1||}
$$

~~~python
import numpy as np
def power_iter(A, x_0, p):
  # A: nxn matrix, x_0: initial guess, p: type of norm
  x_0 = x_0/np.linalg.norm(x_0,p)
  x_k = x_0
  for i in range(max_iterations):
    y_k = A @ x_k
    x_k = y_k/np.linalg.norm(y_k,p)
  return x_k
~~~

