# CS 357

## Errors and Complexity

#### Absolute and Relative Error

We define that: Approximate Result = True Value + Error. Then, we can get our absolute error:
$$
Absolute Error = \abs{x-\hat{x}}.
$$
And, the relative error:
$$
Relative Error = \frac{Absolute Error}{\abs{x}} =\frac{\abs{x-\hat{x}}}{\abs{x}}
$$

#### Significant Digits/Figures

**Significant figures** of a number are digits that carry meaningful information. We define an approximate result $\hat{x}$ has $n$ **significant figures** of a true value $x$ if the absolute error has zeros in the first n decimal place counting from the leftmost nonzero (leading) digit pf $x$, followed by a digit from 0 to 4. Here is an example.

Assume $x=3.141592653$ and suppose $\hat{x}$ is the approximate result:
$$
\hat{x} = 3.14159 \rarr \abs{x-\hat{x}}=0.000002653\rarr\hat x \text{ has 6 significant figures}.
$$

$$
\hat{x} = 3.1415 \rarr \abs{x-\hat{x}}=0.000092653\rarr\hat x \text{ has 4 significant figures}.
$$

## Floating Point

#### Floating Point Numbers

A floating-point number can represent numbers of different orders of magnitude(very large and very small) with the same number of fixed digits.

More formally, we can define a floating-point number $x$ as:
$$
x=±q⋅2^m
$$


where:

- ± is the sign
- q is the significand
- m is the exponent

A number xx in a normalized binary floating-point system has the form
$$
x =±1.b_1b_2b_3\dots b_n \times 2^m = ±1.f \times2^m
$$
where **Digits**: $b_i \in {0,1}$,**Exponent range:** Interger $m\in [L,U]$, **Precision:** $p=n+1$, **Smallest positive normalized floating-point number:**$2^L$, **Largest positive normalized floating-point number:**$2^{U+1}(1-2^{-p})$

Outside the range will be consider as **overflow**.

#### Machine Epsilon

**Machine epsilon** ($\epsilon_m$) is defined as the distance (gap) between 1 and the next largest floating point number. It is do not depend on the exponent. 
$$
\epsilon_m = 2^{-n}
$$
