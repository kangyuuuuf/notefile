# CS 357

## Errors and Complexity

#### Absolute and Relative Error

We define that: Approximate Result = True Value + Error. Then, we can get our absolute error:
$$
Absolute Error = |{x-\hat{x}}|.
$$
And, the relative error:
$$
Relative Error = \frac{Absolute Error}{|{x}|} =\frac{|{x-\hat{x}}|}{|{x}|}
$$

#### Significant Digits/Figures

**Significant figures** of a number are digits that carry meaningful information. We define an approximate result $\hat{x}$ has $n$ **significant figures** of a true value $x$ if the absolute error has zeros in the first n decimal place counting from the leftmost nonzero (leading) digit pf $x$, followed by a digit from 0 to 4. Here is an example.

Assume $x=3.141592653$ and suppose $\hat{x}$ is the approximate result:
$$
\hat{x} = 3.14159 \rarr |{x-\hat{x}}|=0.000002653\rarr\hat x \text{ has 6 significant figures}.
$$

$$
\hat{x} = 3.1415 \rarr |{x-\hat{x}}|=0.000092653\rarr\hat x \text{ has 4 significant figures}.
$$

## Floating Point

#### Floating Point Numbers

A floating-point number can represent numbers of different orders of magnitude(very large and very small) with the same number of fixed digits.

More formally, we can define a floating-point number $x$ as:
$$
x=±q⋅2^m
$$


where:

- ± is the sign
- q is the significand
- m is the exponent

A number xx in a normalized binary floating-point system has the form
$$
x =±1.b_1b_2b_3\dots b_n \times 2^m = ±1.f \times2^m
$$
where **Digits**: $b_i \in {0,1}$,**Exponent range:** Integer $m\in [L,U]$, **Precision:** $p=n+1$, **Smallest positive normalized floating-point number:** $2^L$, **Largest positive normalized floating-point number:** $2^{U+1}(1-2^{-p})$

Outside the range will be considered as **overflow**.

#### Machine Epsilon

**Machine epsilon** ($\epsilon_m$) is defined as the distance (gap) between 1 and the next largest floating-point number. It does not depend on the exponent. 
$$
\epsilon_m = 2^{-n}
$$

## Rounding and Cancellation

Consider a real number in normalized floating-point form:
$$
x=\pm 1.b_1b_2b_3\dots b_n\dots\times 2^m
$$
Without loss of generality, let us assume $x$ is a positive number. In this case, we have:
$$
x_-=\pm 1.b_1b_2b_3\dots b_n\times 2^m
$$

$$
x_+=\pm 1.b_1b_2b_3\dots b_n\times 2^m + 0.\text{(n-1 bits of 0)1} \times 2^m
$$

![rounding_table](CS-357.assets/rounding_table.png)

#### Roundoff Errors

The difference between $x_-$ and $x_+$ is  $\epsilon_m \times 2^m$.

Hence we can use machine epsilon to bound the error in representing a real number as a machine number.

**Absolute error:**
$$
|{fl(x)-x}| \le |{x_+-x_-}| = \epsilon_m \times 2^m
$$
**Relative error:**
$$
\frac{|{fl(x)-x}|}{|{x}|} \le \frac{\epsilon_m \times 2^m}{|{x}|}
$$

#### Floating Point Addition

The basic idea of adding is 

1.  Bring both numbers to a common exponent
2. Do grade-school addition from the front, until you run out of digits in your system
3. Round the result

There is no loss of significant digits with floating-point addition.

#### Floating Point Subtraction and Cancellation

Floating-point subtraction works much the same way that addition does. However, problems occur when you subtract two numbers of similar magnitude. There is an example
$$
a = 1.1011???? \times 2^1,b = 1.1010???? \times 2^1
$$

$$
a-b =0.0001???? \times 2^1
$$

Although the floating-point number will be stored with 4 digits in the fractional, it will only be accurate to a single significant digit. This loss of significant digits is known as **catastrophic cancellation**.

## Taylor Series

#### Infinite Taylor Series Expansion

A Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function’s derivatives at a single point. The Taylor series expansion about $x=x_0$ of a function $f(x)$ that is infinitely differentiable at $x_0$ is the power series
$$
f(x_0)+\frac{f'(x_0)}{1!}(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2+\frac{f'''(x_0)}{3!}(x-x_0)^3 + \dots
$$
Then, we can use summation notation 
$$
\sum_{k=0}^{\infin}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k.
$$

#### Error Bound when Truncating a Taylor Series

Suppose that $f(x)$ is an $n+1$ times differentiable function of $x$, and $T_n(x)$ is the Taylor polynomial of degree $n$ for $f(x) $ centered at $x_0$. Then When $h=|{x-x_0}| \rarr0$, we obtain the truncation error bound by
$$
|{f(x)-T_n(x)}|\le C \cdot h^{n+1}=O(h^{n+1})
$$

#### Taylor Remainder Theorem

Suppose that $f(x)$ is an $n_1$ times differentiable function of $x$. Let $R_n(x)$ denote the difference $f(x)$ and the Taylor polynomial of degree $n$ for $f(x)$ centered at $x_0$. Then
$$
R_n(x) = f(x)-T_n(x)= \frac{f^{n+1}(\mathcal{E})}{(n+1)!}(x-x_0)^{n+1}
$$
for some $\mathcal{E}$ between $x$ and $x_0$. Thus, the constant $C$ mentioned above is 
$$
\max_{\cal E} \frac{|{f^{(n+1)}(\cal E)}|}{(n+1)!}
$$

#### Asymptotic behavior of the error

Let’s say we have $f(x)$ approximated using $t_n(x)$. Suppose the given interval is h1h1 between $x_0$ and $x$ and the error associated with it is $e_1$. Let’s say we have another interval $h_2$ and we need to find the error $e_2$ associated with it.

Using the formula $e=O(h^{n+1})$, we get
$$
e_1 \propto h_1^{n+1}\\
e_2 \propto h_2^{n+1}\\
\frac{e_1}{e_2} = (\frac{h_1}{h_2})^{n+1}\\
e_2=(\frac{h_2}{h_1})^{n+1}e_1
$$

#### Implement Taylor Series in Python

Here we need to import **sympy**, a package for symbolic computation with python.

```python
import sympy as sp
# We need to declear a variable name x
sp.var("x")
# Then, the x will become a variable instead of a char or name.
# We can create a function with variable x.
g = sp.sin(sp.sqrt(x) + 2) ** 2 
# Take a derivative, using .diff(x,the time of diff)
g.diff(x,2)
# Use .sub(x,...) and .evalf() to evaluate the expression for x = 1.
g.subs(x,1).evalf()
# print 0.019914856674817
```

After knowing the basic function of simply, we can implement our Taylor function:

~~~python
def taylor_ser(n,x0,f):
  tn = 0;
  for i in range(n+1):
      tn += f.diff(x, i).subs(x, x0)/factorial(i) * (x-x0)**i
  return tn;
# find the apporixmation
def taylor_appox(value,f):
  return f.subs(x,value).evalf()
~~~

## Random Numbers and Monte Carlo Methods

#### Randomness

```python
import numpy as np

np.random.seed(seed)						# fix the seed of generator
np.random.randint(low,high)			# random integer 
np.random.rand(num)							#	random float number within 0 to 1

np.random.choice([0,1])					# random 0,1 
```

#### Linear Congruential Generator

A **linear congruential generator** (LCG) is pseudorandom number generator of the form:
$$
x_k = (ax_{k-1}+c)\mod M
$$
where $a$ and cc are given integers and $x0$ is called the **seed**. The period of an LCG cannot exceed $M$. The quality depends on both aa and $c$, and the period may be less than $M$ depending on the values of $a$ and $c$.

#### Monte Carlo Methods

**Monte Carlo methods** are algorithms that rely on repeated random sampling to approximate a desired quantity. Monte Carlo methods are typically used in modeling the following types of problems:

- Nondeterministic processes
- Complicated deterministic systems and deterministic problems with high dimensionality (e.g., Monte Carlo integration)

 Therefore, the asymptotic behavior of the Monte Carlo method is $O(\frac{1}{\sqrt{n}})$ where nn is the number of samples.
$$
err \rarr \frac{1}{\sqrt{n}}Z
$$

## Vector, Matrices, and Norms

The concept can follow the link [Course Note](https://courses.grainger.illinois.edu/cs357/sp2022/notes/ref-8-vec-mat.html). 

## Linear System of Equation

 

Useful libraries for this notebook:

```PYTHON
import numpy as np
import numpy.linalg as la
import scipy.linalg as sla
import scipy.sparse as sparse


import matplotlib.pyplot as plt
%matplotlib inline

from PIL import Image

import seaborn as sns
sns.set(font_scale=2)
plt.style.use('seaborn-whitegrid')

from time import time
```

#### Transforming images using linear operators

```python
img = Image.open('ssn.png')
xmat = (255 - np.asarray(img).max(axis=2))/255

x = xmat.flatten()
x.shape # The shape is (x,)

```

We can construct a “blur ” matrix, this will blur our picture

```python
def blur_operator(m,n,radius):
    imat, jmat = np.meshgrid(np.arange(m), np.arange(n), indexing='ij')
    ivec = np.atleast_2d(imat.flatten())
    jvec = np.atleast_2d(jmat.flatten())
    A = np.fmax(0, 1 - np.sqrt((ivec.T - ivec)**2 + (jvec.T - jvec)**2)/radius)
    A /= A.sum(axis=1)
    return A
```

~~~python
A = blur_operator(xmat.shape[0],xmat.shape[1],5)
A.shape #(x,x)
~~~

Know, we have blur picture b, we know the transform matrix A, we can get our picture back using **np.linalg.solve**

```python
x_solve1 = la.solve(A,b)
x_solve1.shape
```

We can add some noise to our blur picture, this will make the process not invertible.

~~~python
b_noisy = b + 1e-3 * np.random.rand(b.size)

x_noisy = la.solve(A,b_noisy)

plt.imshow(x_noisy.reshape(xmat.shape))
~~~

#### Solving Triangular Systems

We have forward-substitution and backward-substitution. Using substitution, the running time of calculation is faster than directly finding the solution. Although the running time of finding LU takes more time than finding a solution, it is extremely useful when we need to calculate different variables when we have the same matrix.

```python
def my_forward_substitution(M,b):
    n = len(b)
    y = np.zeros(n)
    for i in range(n):
        tmp = b[i]
        for j in range(i):
            tmp -= y[j]*M[i,j]
        y[i]=tmp/M[i,i]
    return y

def my_backward_substitution(M,b):
    n = len(b)
    y = np.zeros(n)
    for i in range(n-1, -1, -1):
        tmp = b[i]
        for j in range(i+1, n):
            tmp -= y[j]*M[i,j]
        y[i] = tmp/M[i,i]
    return y
```

Using scipy, we can get the LU decomposition. Using **solve_triangular**,  we can get the solution of triangular matrix

```python
import scipy.linalg as sla
P, L, U = sla.lu(A)

y = sla.solve_triangular(L, P.T@b, lower=True)

x_solve = sla.solve_triangular(U, y)
```

