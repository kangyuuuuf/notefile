# CS 357

## Errors and Complexity

#### Absolute and Relative Error

We define that: Approximate Result = True Value + Error. Then, we can get our absolute error:
$$
Absolute Error = |{x-\hat{x}}|.
$$
And, the relative error:
$$
Relative Error = \frac{Absolute Error}{|{x}|} =\frac{|{x-\hat{x}}|}{|{x}|}
$$

#### Significant Digits/Figures

**Significant figures** of a number are digits that carry meaningful information. We define an approximate result $\hat{x}$ has $n$ **significant figures** of a true value $x$ if the absolute error has zeros in the first n decimal place counting from the leftmost nonzero (leading) digit pf $x$, followed by a digit from 0 to 4. Here is an example.

Assume $x=3.141592653$ and suppose $\hat{x}$ is the approximate result:
$$
\hat{x} = 3.14159 \rarr |{x-\hat{x}}|=0.000002653\rarr\hat x \text{ has 6 significant figures}.
$$

$$
\hat{x} = 3.1415 \rarr |{x-\hat{x}}|=0.000092653\rarr\hat x \text{ has 4 significant figures}.
$$

## Floating Point

#### Floating Point Numbers

A floating-point number can represent numbers of different orders of magnitude(very large and very small) with the same number of fixed digits.

More formally, we can define a floating-point number $x$ as:
$$
x=±q⋅2^m
$$


where:

- ± is the sign
- q is the significand
- m is the exponent

A number xx in a normalized binary floating-point system has the form
$$
x =±1.b_1b_2b_3\dots b_n \times 2^m = ±1.f \times2^m
$$
where **Digits**: $b_i \in {0,1}$,**Exponent range:** Integer $m\in [L,U]$, **Precision:** $p=n+1$, **Smallest positive normalized floating-point number:** $2^L$, **Largest positive normalized floating-point number:** $2^{U+1}(1-2^{-p})$

Outside the range will be considered as **overflow**.

#### Machine Epsilon

**Machine epsilon** ($\epsilon_m$) is defined as the distance (gap) between 1 and the next largest floating-point number. It does not depend on the exponent. 
$$
\epsilon_m = 2^{-n}
$$

## Rounding and Cancellation

Consider a real number in normalized floating-point form:
$$
x=\pm 1.b_1b_2b_3\dots b_n\dots\times 2^m
$$
Without loss of generality, let us assume $x$ is a positive number. In this case, we have:
$$
x_-=\pm 1.b_1b_2b_3\dots b_n\times 2^m
$$

$$
x_+=\pm 1.b_1b_2b_3\dots b_n\times 2^m + 0.\text{(n-1 bits of 0)1} \times 2^m
$$

![rounding_table](CS-357.assets/rounding_table.png)

#### Roundoff Errors

The difference between $x_-$ and $x_+$ is  $\epsilon_m \times 2^m$.

Hence we can use machine epsilon to bound the error in representing a real number as a machine number.

**Absolute error:**
$$
|{fl(x)-x}| \le |{x_+-x_-}| = \epsilon_m \times 2^m
$$
**Relative error:**
$$
\frac{|{fl(x)-x}|}{|{x}|} \le \frac{\epsilon_m \times 2^m}{|{x}|}
$$

#### Floating Point Addition

The basic idea of adding is 

1.  Bring both numbers to a common exponent
2. Do grade-school addition from the front, until you run out of digits in your system
3. Round the result

There is no loss of significant digits with floating-point addition.

#### Floating Point Subtraction and Cancellation

Floating-point subtraction works much the same way that addition does. However, problems occur when you subtract two numbers of similar magnitude. There is an example
$$
a = 1.1011???? \times 2^1,b = 1.1010???? \times 2^1
$$

$$
a-b =0.0001???? \times 2^1
$$

Although the floating-point number will be stored with 4 digits in the fractional, it will only be accurate to a single significant digit. This loss of significant digits is known as **catastrophic cancellation**.

## Taylor Series

#### Infinite Taylor Series Expansion

A Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function’s derivatives at a single point. The Taylor series expansion about $x=x_0$ of a function $f(x)$ that is infinitely differentiable at $x_0$ is the power series
$$
f(x_0)+\frac{f'(x_0)}{1!}(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2+\frac{f'''(x_0)}{3!}(x-x_0)^3 + \dots
$$
Then, we can use summation notation 
$$
\sum_{k=0}^{\infin}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k.
$$

#### Error Bound when Truncating a Taylor Series

Suppose that $f(x)$ is an $n+1$ times differentiable function of $x$, and $T_n(x)$ is the Taylor polynomial of degree $n$ for $f(x) $ centered at $x_0$. Then When $h=|{x-x_0}| \rarr0$, we obtain the truncation error bound by
$$
|{f(x)-T_n(x)}|\le C \cdot h^{n+1}=O(h^{n+1})
$$

#### Taylor Remainder Theorem

Suppose that $f(x)$ is an $n_1$ times differentiable function of $x$. Let $R_n(x)$ denote the difference $f(x)$ and the Taylor polynomial of degree $n$ for $f(x)$ centered at $x_0$. Then
$$
R_n(x) = f(x)-T_n(x)= \frac{f^{n+1}(\mathcal{E})}{(n+1)!}(x-x_0)^{n+1}
$$
for some $\mathcal{E}$ between $x$ and $x_0$. Thus, the constant $C$ mentioned above is 
$$
\max_{\cal E} \frac{|{f^{(n+1)}(\cal E)}|}{(n+1)!}
$$

#### Asymptotic behavior of the error

Let’s say we have $f(x)$ approximated using $t_n(x)$. Suppose the given interval is h1h1 between $x_0$ and $x$ and the error associated with it is $e_1$. Let’s say we have another interval $h_2$ and we need to find the error $e_2$ associated with it.

Using the formula $e=O(h^{n+1})$, we get
$$
e_1 \propto h_1^{n+1}\\
e_2 \propto h_2^{n+1}\\
\frac{e_1}{e_2} = (\frac{h_1}{h_2})^{n+1}\\
e_2=(\frac{h_2}{h_1})^{n+1}e_1
$$

#### Implement Taylor Series in Python

Here we need to import **sympy**, a package for symbolic computation with python.

```python
import sympy as sp
# We need to declear a variable name x
sp.var("x")
# Then, the x will become a variable instead of a char or name.
# We can create a function with variable x.
g = sp.sin(sp.sqrt(x) + 2) ** 2 
# Take a derivative, using .diff(x,the time of diff)
g.diff(x,2)
# Use .sub(x,...) and .evalf() to evaluate the expression for x = 1.
g.subs(x,1).evalf()
# print 0.019914856674817
```

After knowing the basic function of simply, we can implement our Taylor function:

~~~python
def taylor_ser(n,x0,f):
  tn = 0;
  for i in range(n+1):
      tn += f.diff(x, i).subs(x, x0)/factorial(i) * (x-x0)**i
  return tn;
# find the apporixmation
def taylor_appox(value,f):
  return f.subs(x,value).evalf()
~~~

## Random Numbers and Monte Carlo Methods

#### Randomness

```python
import numpy as np

np.random.seed(seed)						# fix the seed of generator
np.random.randint(low,high)			# random integer 
np.random.rand(num)							#	random float number within 0 to 1

np.random.choice([0,1])					# random 0,1 
```

#### Linear Congruential Generator

A **linear congruential generator** (LCG) is pseudorandom number generator of the form:
$$
x_k = (ax_{k-1}+c)\mod M
$$
where $a$ and cc are given integers and $x0$ is called the **seed**. The period of an LCG cannot exceed $M$. The quality depends on both aa and $c$, and the period may be less than $M$ depending on the values of $a$ and $c$.

#### Monte Carlo Methods

**Monte Carlo methods** are algorithms that rely on repeated random sampling to approximate a desired quantity. Monte Carlo methods are typically used in modeling the following types of problems:

- Nondeterministic processes
- Complicated deterministic systems and deterministic problems with high dimensionality (e.g., Monte Carlo integration)

 Therefore, the asymptotic behavior of the Monte Carlo method is $O(\frac{1}{\sqrt{n}})$ where nn is the number of samples.
$$
err \rarr \frac{1}{\sqrt{n}}Z
$$

## Vector, Matrices and Norms

The note will focus Python part. The concept can follow the link [Course Note](https://courses.grainger.illinois.edu/cs357/sp2022/notes/ref-8-vec-mat.html). 
