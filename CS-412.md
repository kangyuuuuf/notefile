# CS 412- Data and Information Systems

#### Data Mining: On what kinds of data?

-   Database-iruebted data sets and applications
    -   Relational database, data warehouse, transactional database
    -   Object-relational databases, Heterogeneous databases, and legacy databases
-   Advanced data sets and advanced applications
    -   Data steans abd sebsir data
    -   Tune-series data, tenoiral data, sequence data
    -   Structure data, graphs, social networks, and information networks
    -   Spatial data and spatiotemporal data
    -   Multimedia database
    -   Text databases
    -   The World-Wide Web

### Data, Measyrenebts, and Data Preprocessing

#### Recorded Data

-   Relational Records
    -   Relational tables, highly structured 
-   Data natrix, e.g., numerical matrix, crosstabs
-   Transaction Data
-   Document data: Term-frequency vector (matrix) of text documents

#### Graph and Networks

-   Transportation network
-   World Wide Web
-   Molecular Structures
-   Social or information networks

#### Ordered Data

-   Video Data: sequence of images
-   Temporal data: time-series
-   Sequential Data: transaction sequences
-   Genetic sequence data

#### Spatial, image and multimedia Data

-   Spatial data
-   Image data
-   Video data

#### Data Object

Data sets are made up of data objects, and a data object represents an entity

#### Attributes

It has many types: 

-   Nominal: categories, states, or “names of things”
    -   Color
    -   marital status, iccupation, ID numbers, zip codes
-   Binary
    -   Nominal attri bute with only 2 states
    -   Symmetric binary: outcomes are equally important/Asymmetric Binary: outcomes not equally important
-   Ordinal, 
-   Numeric: quantitative
    -   Interval-scaled: Measure on a scale of equal-sized unit, valyues have order, no true zero-point: e.g 100 $^oC$
    -   Ratio-scaled: Inherent zero-point, we can speak of values as being an order of magnitude larger than the unit of measurement, 100 $^oK$

#### Basic Statistical Description of Data

**Motivation**: it is to better understand the data: central tendency, variation and spread

**Data dispersion characteristics**: median, max, min, quantiles, outliers, cariance
$$
median = L_1 + (\frac{n/2-(\sum freq)_l}{freq_{median}} )width
$$

#### Histogram Analysis

Difference between histograms and bar charts

-   Histograms are used to show distributions of variables while bar charts are used to compare variables\
-   Histograms plot binned quantitative data which bar charts plot categorical data
-    Bars can be reordered in bar charts but not in histograms

#### Quantile Plot

For a data $x_i$ sorted in increasing order, $f_i$ indicates that approximately 100 $f_i \%$ if the data are below or equal to the value $x_i$.

#### Cosine Similarity of Two Vectors

Cosine measure: if $d_1$ and $d_2$ are two vectors, then
$$
\cos(d_1,d_2) = \frac{d_1 \cdot d_2}{||d_1|| \times ||d_2||}
$$
We can calculate the similarity by finding the $\theta$ between two vectors. When the $\theta$ is small, it means two vectors are in about the same direction, meaning they are more similar.

#### Correlation Analysis

$x^2$ test: (where $O_i$ is observed and $E_i$ is expected)
$$
\chi^2 = \sum^n_i \frac{(O_i-E_i)^2}{E_i}
$$


Null hypothesis: the two distributions are independent.

The lager $\chi^2$, the higher chance the variable are related. Note: correlation does not imply causality. 

#### Variance for Single Variable

The variance of a random variable $X$ provides a measure of how much the value of $X$ deviates from the mean of the expected value of $X$.
$$
\sigma^2 = var(X) = E[(x-\mu)^2] = \{\begin{matrix} 
\sum_x (x-\mu)^2 f(x) \qquad \text{if $X$ is discrete}\\
\int_{-\infin}^\infin (x-\mu)^2 f(x)dx \qquad \text{if $X$ is continuous}
\end{matrix}
$$
Covariance between two variables $X_1$ and $X_2$
$$
\sigma_{12} = E[(X_1-\mu_1)(X_2-\mu_2)] = E[X_1X_2] - \mu_1\mu_2 = E[X_1X_2]-E[X_1]E[X_2]
$$
Sample covariance between $X_1$ and $X_2$: 
$$
\hat \sigma_{12} = \frac{1}{n-1}\sum_{i=1}^n (x_{i1}-\hat \mu_1)(x_{i2}-\hat \mu_2)
$$
WIt is a generalization of the sample variance:
$$
\hat \sigma_{11} = \frac{1}{n-1}\sum_{i=1}^n (x_{i1}-\hat \mu_1)(x_{i1}-\hat \mu_1)
$$
Covariance can be negative!

#### Correlation

Correlation between two variables $X_1$ and $X_2$ is the standard covariance obtained by normalizing the covariance with the standard deviation of each variable
$$
\rho_{12} = \frac{\sigma_{12}}{\sigma_1\sigma_2}
$$


Correlation > 0 means positively correlated 

Correlation = 0 means  uncorrelated 

Correlation < 0 means negatively correlated 

#### Covariance Matrix

$$
\Sigma = E[(X- \mu)(X-\mu)^T]
$$

#### KL Divergence: Comparing Two Probability Distribution

$$
D_{KL} (p(x)||q(x)) = \sum _{x\in X} p(x)\ln\frac{p(x)}{q(x)}
$$

$$
D_{KL} (p(x)||q(x)) = \int _{-\infin}^\infin p(x)\ln\frac{p(x)}{q(x)}dx
$$

#### Types of Sampling

-   Simple random sampling
-   Sampling without replacement
-   Sampling with replacement 
-   Stratified sampling

#### Dimensionality Reduction

Curse of dimensionality

#### Principal Component Analysis

PCA: A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables.

The original data are projected onto a much smaller space, resulting in dimensionality reduction

Method: FInind the eigenvectors of the covariance matrix, and these eigenvectors define the new space.

#### Data Warehouse

##### OLTP VS OLAP

OLTP: Online tracsactional processing

-   DBMS operations
-   Query and transactional processing

OLAP: online analytical proce s sing

-   Data warehouse operation
-   Drilling, slicing, dicing, etc.

## Mining Frequent Patterns. Associations and Correlations

**Patterns:** A set of items, subsequences, or substructures that occur frequently together. Patterns represent intrinsic and important properties of datasets.

#### Transactional Database

-   Each transaction is associated with an identifier called a TID.
-   May also have counts associated with each item sold.

**Itemset:** A set of one or more items. $I = \{I_1,I_2, \cdots,Ii_m\} $

**k_timeset:** An tiemset containing k items.

**Absolute support(count):** sup{X} = occurrences of an itemset X.

**Relative support** s{X} = The fraction of transactions that contains X.

An item set X is **frequent** if the support of X is no less than a minsup threshold $\sigma$

$(X \rightarrow Y)$The conditional probability that a transaction containing X also contains Y:
$$
c = \frac{sup(X,Y)}{sup(X)} \\
s = \frac{sup(X,Y)}{X}
$$

#### Association Rules

Association rule mining 

-   Given two thresholds: minsup, minconf
-   Find all of the rules, $X \rightarrow Y(s,c)$ such that $s \ge$ minsup and $c \ge$ minconf

#### Patterns

**Closed patterns: ** A pattern (itemset) X is closed if X is **frequent**, and there exists no super-pattern $X \sub Y$, with the same support as X. It is a **lossless compression** of frequent patterns.

**Max patterns:** A pattern is a max-pattern if X is frequent and there exists no frequent super-pattern $X \sub Y$. It is a **lossy compression**.

### Efficient Pattern Mining Methods

#### The Downward Closure Property of Frequent Patterns

**Downward closure(Apropori):** Any subset of a frequent itemset must be frequent.,

If Any subset of an itemset S is **infrequent**, then there is no chance for S to be frequent. 

#### Apriori

1.  Scan DB once to get frequent 1-itemset, k = 1
2.  Loop
    1.  Generate length-(k+1) candidate itemsets from length-k frequent itemsets
    2.  test the candidates against DB to find frequent (k+1)-itemsets
    3.  set k = k+1
3.  End if no frequent or candidate set can be generated
4.  Return all the frequent itemsets derived

**Partitioning**: Scan DB twice. 

-   Scan 1: partition database so that each partition can fit in main memory
-   Scan 2: consolidate global frequent patterns

**ECLAT:** A depth-first search algorithm using set intersection

#### FPGrowth

1.  Scan DB once, find single item frequent parttern
2.  Sort frequent items in frequency descending order
3.  Scan DB again, find the ordered frequent itemlist for each transcation
4.  For each transcation, insert the ordered frequent itemlist into an FP-tree, with shared sub-branches merged, counts accumulated.

### Pattern Evaluation

Measure of dependent/correlated events: **lift**
$$
lift(B,C) = \frac{c(B\rightarrow C)}{s(C)} = \frac{s(B \cup C)}{s(B) \times s(C)}
$$

-   Lift(B,C) = 1: independent
-   $> 1$:  positively correlated
-   $< 1:$ negatively correlated

Measure to test correlated events:
$$
X^2 =\sum\frac{(Observed - Expected)^2}{Expected}
$$

#### Imbalance Ratio

IR: measure the imbalance of two itemsets A and B in rule of implications. 
$$
IR(A,B) = \frac{|s(A)-s(B)|}{s(A)+s(B)-s(A\cup B)}
$$

### Advanced Pattern Mining

A rult is redundant if its support is close to the expected value, according to its ancestor rule, and it has a similar confidence as its ancestor

**Single-dimensional rules**: all the items are in the same dimension

**Multi-dimensional rules**: the items are from different  dimension

#### Mining Extraordinary Phenomena

E.g: Gender = female $\implies$ Wage: mean = 7 (overall mean = 9)

LHS: a subset of the population

RHS: amm extraordinary behavior of this subset

The rule is accepted only if a statistical test(e.g. Z-test) confrims the inference with high confidence

**Subrule:** Highlights the extraordinary behavior of a subset of the popilation of the super rule

E.g: (Gender = female) $\and$ (South = Yes) $\implies$ Wage: mean = 7 (overall mean = 9)

#### Negative Patterns

It just means unlikely to happen together

If itemsets A and B are both frequent but rarely occur together, i.e., $s(A\cup B)\ll s(A) $

#### Null-Invariance

Sometimes when the transcation becomes very large, the ratio of $s(A\cup B) $ and $ s(A) $ become smaller. To aviod this situation happens, we have

**Kulczynski measure-based definition:** If itemsets A and B are frequent but $(s(A\cup B)/s(A)+ s(A\cup B)/s(B))/2 < \epsilon$, then A and B are negatively correlated.

### Mining Compressed Patterns

Pattern Distance measure: $Dist(P_1,P_2) = 1 - \frac{|T(P_1) \cap T(P_2)|}{|T(P_1) \cup T(P_2)|}$.

$\delta-$clustering: For each pattern P find all patterns which can be expressed by P and whose distance to P is within $\delta$.

### Constraint-Based Pattern Mining

A constraint c is anti-monotone if an itemset S violates constraint c, so does any of its supersets. $sum(S.price)<v$ is anti-monotone. $sum(S.price)\ge v$ is **not** anti-monotone. 

A constraint c is monotone if an itemset S satisfies constraint c, so does any of its supersets. 

A constraint c is data anti-monotone:in the mining process, if a data entry $t$ cannot contribute to a pattern $p $ satisfying $c,t$ cannot contribute to $p$’s superset either.

#### Succincness

If the constraint c can be enforced by directly manipiating the data

