

# CS 444

These are the required topics for this course. Finished assignments will be stored inside [CS-444.assets folder](./CS-444.assets).

| **Progress** | **Topic**                                                    | **Assignments**                                              |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
|              | Convolutional networks: **[PPTX](https://slazebni.cs.illinois.edu/spring23/lec07_cnn.pptx)**, **[PDF](https://slazebni.cs.illinois.edu/spring23/lec07_cnn.pdf)** | **[Assignment 2](https://slazebni.cs.illinois.edu/spring23/assignment2.html)** |
| In progress  | PyTorch tutorial                                             | **[Jupyter notebook](.//CS-444.assets/slazebni.cs.illinois.edu_spring23_pytorch_tutorial_cs444sp23.ipynb)** |
|              | Object detection: **[PPTX](https://slazebni.cs.illinois.edu/spring23/lec10_detection.pptx)**, **[PDF](https://slazebni.cs.illinois.edu/spring23/lec10_detection.pdf)** |                                                              |
|              | Dense prediction: **[PPTX](https://slazebni.cs.illinois.edu/spring23/lec11_dense.pptx)**, **[PDF](https://slazebni.cs.illinois.edu/spring23/lec11_dense.pdf)** | **[Assignment 3](https://slazebni.cs.illinois.edu/spring23/assignment3.html)** |

## PyTorch Tutorial

### PyTorch Tensors

Just like `Numpy` arrays

```python
x = torch.rand(5, 3) 
x = torch.zeros(5, 3, dtype=torch.long)
x = torch.tensor([1, 2, 3])
x.dtype
x = x.new_ones(5, 3) # will return the same type and device
```

A `torch.device` is an object representing the device on which a torch.Tensor is or will be allocated.

```python
x = torch.randn_like(x, dtype=torch.float)  
```

Operations on tensors just like `Numpy`

```python
x = torch.ones(5, 3)
y = torch.ones(5, 3)
print("x + y:", x + y)
print("torch.add(x, y):", torch.add(x, y))
print("x:", x)
y.add_(x) # directly affect y
print(y)

x = torch.randn(5, 3, 2)
print(x.shape)
print(x.permute(1,2,0).shape) # change the shape of the torch

a = torch.ones(3, 3, dtype=torch.double)
print(a)
b = a.long()
print(b.type())
c = a.int()
print(c) # change the type and casting
```

#### Broadcasting Semantics

Two tensors are broadcastable if

- both tensor have at least one dimension
- When iterating, the dimension sizes must either be **equal**, **one of them is 1**, or **one of them don’t exist**

Here are the example from [this link](https://pytorch.org/docs/stable/notes/broadcasting.html)

```python
x=torch.empty(5,7,3)
y=torch.empty(5,7,3)
# same shapes are always broadcastable (i.e. the above rules always hold)

x=torch.empty((0,))
y=torch.empty(2,2)
# x and y are not broadcastable, because x does not have at least 1 dimension

# can line up trailing dimensions
x=torch.empty(5,3,4,1)
y=torch.empty(  3,1,1)
# x and y are broadcastable.
# 1st trailing dimension: both have size 1
# 2nd trailing dimension: y has size 1
# 3rd trailing dimension: x size == y size
# 4th trailing dimension: y dimension doesn't exist

# but:
x=torch.empty(5,2,4,1)
y=torch.empty(  3,1,1)
# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3
```

If two tensors $x,y$ are broadcastable, then the result size of the tensor will be calculated as follows

- if the number of dimensions of `x` and `y` are not equal, perpend 1 to the dimension of fewer-dimension tensor.
- Then, for each dimension size, the resulting dimension size is`max(x-dim,y-dim)` (along that dimension)

```python
x=torch.empty(5,3,4,1)
y=torch.empty(3,1,1)
(x.add_(y)).size()
torch.Size([5, 3, 4, 1])

# but:
x=torch.empty(1,3,1)
y=torch.empty(3,1,7)
(x.add_(y)).size()

# RuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2.
```

#### **More useful PyTorch Tensor operations**

`.view()` function can resize/reshape tensors

```python
x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  # the size -1 is inferred from other dimensions
print(x.size(), y.size(), z.size())
```

`.item()` give a python number

 `torch.cat()` concatenating two matrices together

```python
x = torch.ones(5, 3)
y = torch.zeros(5, 2)
print(torch.cat([x, y], dim=1))
# wrong:
# print(torch.cat([x, y], dim=0))
```

####  Numpy arrays $\Longleftrightarrow$ PyTorch Tensors

Note: both arrays will share the same address/location. Changing one of them will change both.

 ```python
 a = torch.ones(5)
 b = a.numpy()
 a.add_(1) # change both
 
 a = np.ones(5)
 b = torch.from_numpy(a)
 np.add(a, 1, out=a) # change both
 ```

#### CUDA Tensors (on GPU)

PyTorch tensors have the added benefit that they can easily be placed on a GPU to speed up computations.

If CUDA is available, these is the Query information about the GPU,

```python
if torch.cuda.is_available():
    !nvidia-smi
```

`torch.device` use to move tensors to and from the GPU

```PYTHON
if torch.cuda.is_available():
    device = torch.device("cuda")          # a CUDA device object
    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU
    x = x.to(device)                       # or just use strings `.to("cuda")`
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))       # `.to` can also change dtype
```

`torch.cuda.set_device(device)` set the default device

### Autograd: Automatic Differentiation

It provides automatic differentiation for all operations on Tensors. 

Set `requires_grad ` as True to keep track of operations for automatic differentiation. `.backward()` will return all the gradients computed automatically. `.grad` will accumulate all the gradient for the tensor. To stop, use `.detach()`.

`Function` is a class that use for autograd implementation. Tensor and Function are interconnected, build an acyclic graph that encodes the history of computation. Tensor has `.grad_fn` that references a Function that has created the Tensor.

`.backward()` compute the derivatives on a Tensor.

```python
x = torch.ones(2, 2, requires_grad=True) # no .grad_fn since we create it
y = x + 2	# has .grad_fn
z = y * y * 3	# has .grad_fn
out = z.mean()	# has .grad_fn
```

`.requires_grad_()` changes the existing Tensor’s `requires_grad` flag. Default: false

#### Gradients

```python
out.backward() # same as out.backward(torch.tensor(1))
print(x.grad) # give a matrix of 4.5
```

Using `with torch.no_grad()` to stop from tracking history

```
print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
	print((x ** 2).requires_grad)
```

### Neural Networks

An `nn.Module` contains layers, and a method `forward(input)` that returns the `output`.

A typical training procedure for a neural network is as follows:

- Define the neural network that has some learnable parameters (or
  weights)
- Iterate over a dataset of inputs
- Process input through the network
- Compute the loss (how far is the output from being correct)
- Propagate gradients back into the network’s parameters
- Update the weights of the network, typically using a simple update rule: `weight = weight - learning_rate * gradient`

#### Define the network

Whenever you extend the `nn.Module` class (e.g. with the `Net` class below) you will need to call the superclass constructor or an error will be thrown. In this example below this line is: `super().__init__()` 

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):

    def __init__(self):
        super().__init__()
        # 1 input image channel, 6 output channels, 5x5 convolution kernel
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        # Affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), 2)
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.flatten(start_dim=1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()
print(net)
```

Once we define `forward()` function, `backward()` function will automatically defined.

The learnable parameters of a model are returned by `net.parameters()`

```python
params = list(net.parameters())
print(len(params))
print(params[0].size())  # conv1's .weight

input = torch.randn(1, 1, 32, 32)
out = net(input)	#tyring 32 by 32 input
```

- Zero the gradient buffers of all parameters
- To calculate the gradient of all the parameters that used to compute `out` w.r.t. some random value

```python
net.zero_grad()  # important, since gradient is accumulated
out.backward(torch.randn(1, 10))
```

`nn.Sequential` model can be helpful to define blocks succinctly or avoid creating a new `nn.module` class for a small network. The `.forward()` function will be automatically defined by running modules in the order they are passed in to `nn.Sequential`. 

```python
conv_layers = nn.Sequential(
                nn.Conv2d(1, 6, 5),
                nn.ReLU(),
                nn.Conv2d(6, 16, 5),
                nn.ReLU()
            )
```

#### Loss Function

A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.

![image-20230921223423757](CS-444.assets/image-20230921223423757.png)

```python
output = net(input)
target = torch.randn(10)  # a dummy target, for example
target = target.view(1, -1)  # make it the same shape as output
criterion = nn.MSELoss()

loss = criterion(output, target)
print(loss)

print(loss.grad_fn)  # MSELoss
print(loss.grad_fn.next_functions[0][0])  # Linear
print(loss.grad_fn.next_functions[0][0].next_functions[1][0])  # ReLU
```

#### Backprop

To backpropagate the error all we have to do is call `loss.backward()`. You need to clear the existing gradients though, otherwise the gradients will be accumulated to existing gradients.

```python
net.zero_grad()  # zeroes the gradient buffers of all parameters

print('conv1.bias.grad before backward')
print(net.conv1.bias.grad)

loss.backward()

print('conv1.bias.grad after backward')
print(net.conv1.bias.grad)
```

#### Update the weights

PyTorch has a small package: `torch.optim` that implements all these methods. Using it is very simple:

```python
import torch.optim as optim

# Create your optimizer
optimizer = optim.SGD(net.parameters(), lr=0.01)

# In your training loop:
optimizer.zero_grad()             # zero the gradient buffers
output = net(input)               # compute the forward pass
loss = criterion(output, target)  # compute the loss
loss.backward()                   # compute the gradients
optimizer.step()                  # update the parameters

print(loss)
```

